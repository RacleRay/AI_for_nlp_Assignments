{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN：\n",
    "\n",
    "- 从最简单的增加feature map特征图深度，减少feature map大小的多层卷积网络开始逐步提升性能\n",
    "- AlexNet，开始使用ReLU激活函数，增加了LRN层，并在多GPU上训练提升性能。参数量也随之大幅提升\n",
    "- VGGNet，简化了CNN层的设计，使用3\\*3卷积核增加通道数2倍，使用2\\*2pooling层减小feature map大小为上一层的1/2\n",
    "- Inception，没有了【FC+FC】的连接结构，加入使用不同卷积核组合输出下一层特征图的结构，并且在模型中间计算层也加入了更多的分类误差计算结构\n",
    "- Mobile Net，使用卷积核感受野的原理，在Inception的基础上，将3\\*3卷积变化为，级联的3\\*1+1\\*3卷积，减小了参数量\n",
    "- ResNet，增加了不经过中间卷积计算的skip connection结构，并且每一个子结构都经过BN层，结构中不使用Dropout\n",
    "- ResNeXt，结合了Inception和ResNet，在同一层组合了更多类型的卷积核\n",
    "- DenseNet，将上游输入的Skip Connection连接到下游所有卷积层的输出中\n",
    "- SENet，引入SE Block，通过Global average pooling（Squeeze过程），得到结果经过两层FC（Excitation过程），得到每个channel特征图的重要度，作为权重计算结果。把重要通道的特征强化，非重要通道的特征弱化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T09:01:48.771676Z",
     "start_time": "2019-09-28T09:01:48.718785Z"
    }
   },
   "source": [
    "下面简单看看tf的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T08:02:12.777328Z",
     "start_time": "2019-09-28T08:01:52.253215Z"
    }
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T08:02:22.589182Z",
     "start_time": "2019-09-28T08:02:14.999439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T08:02:30.629006Z",
     "start_time": "2019-09-28T08:02:30.241929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1  # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape(\n",
    "        (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T08:02:33.904245Z",
     "start_time": "2019-09-28T08:02:33.900255Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) /\n",
    "            predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T08:02:40.265228Z",
     "start_time": "2019-09-28T08:02:39.460383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-4dbddd851a97>:55: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5  # kernel size\n",
    "depth = 16  # feature map channels\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size,\n",
    "                                             image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32,\n",
    "                                     shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(\n",
    "        tf.truncated_normal([patch_size, patch_size, num_channels, depth],\n",
    "                            stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer2_weights = tf.Variable(\n",
    "        tf.truncated_normal([patch_size, patch_size, depth, depth],\n",
    "                            stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    layer3_weights = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [image_size // 4 * image_size // 4 * depth, num_hidden],\n",
    "            stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    layer4_weights = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "    # Model.\n",
    "    def model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        conv = tf.nn.conv2d(hidden,\n",
    "                            layer2_weights, [1, 2, 2, 1],\n",
    "                            padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden,\n",
    "                             [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels,\n",
    "                                                logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T08:03:09.588236Z",
     "start_time": "2019-09-28T08:02:50.791108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.093957\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 50: 1.975779\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 43.7%\n",
      "Minibatch loss at step 100: 1.106715\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 64.3%\n",
      "Minibatch loss at step 150: 0.453588\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 200: 0.735939\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 250: 1.131508\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 300: 0.387888\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 350: 0.446959\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 400: 0.281652\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 450: 0.800498\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 500: 0.664015\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 550: 0.953443\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 600: 0.303698\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 650: 0.884597\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 700: 0.958076\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 750: 0.045382\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 800: 0.590306\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 850: 0.860185\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 900: 0.698533\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 950: 0.400615\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 1000: 0.371427\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.6%\n",
      "Test accuracy: 89.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {\n",
    "            tf_train_dataset: batch_data,\n",
    "            tf_train_labels: batch_labels\n",
    "        }\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction],\n",
    "                                        feed_dict=feed_dict)\n",
    "\n",
    "        if (step % 50 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' %\n",
    "                  accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' %\n",
    "                  accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' %\n",
    "          accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (nn.max_pool()) of stride 2 and kernel size 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_pool change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T08:03:22.288919Z",
     "start_time": "2019-09-28T08:03:21.761249Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size,\n",
    "                                             image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32,\n",
    "                                     shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    W_1 = tf.Variable(\n",
    "        tf.truncated_normal([patch_size, patch_size, num_channels, depth],\n",
    "                            stddev=0.1))\n",
    "    b_1 = tf.Variable(tf.zeros([depth]))\n",
    "    W_2 = tf.Variable(\n",
    "        tf.truncated_normal([patch_size, patch_size, depth, depth],\n",
    "                            stddev=0.1))\n",
    "    b_2 = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    W_3 = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [image_size // 4 * image_size // 4 * depth, num_hidden],\n",
    "            stddev=0.1))\n",
    "    b_3 = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    W_4 = tf.Variable(tf.truncated_normal([num_hidden, num_labels],\n",
    "                                          stddev=0.1))\n",
    "    b_4 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "    \n",
    "    # Model.\n",
    "    def model(data):\n",
    "        conv1 = tf.nn.conv2d(data, W_1, [1, 1, 1, 1], padding='SAME')\n",
    "        relu1 = tf.nn.relu(conv1 + b_1)\n",
    "        # dims: batch_size, image_size, image_size, num_channels. \n",
    "        pooling1 = tf.nn.max_pool(relu1, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        \n",
    "        conv2 = tf.nn.conv2d(pooling1, W_2, [1, 1, 1, 1], padding='SAME')\n",
    "        relu2 =  tf.nn.relu(conv2 + b_2)\n",
    "        pooling2 = tf.nn.max_pool(relu2, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        \n",
    "        shape = pooling2.get_shape().as_list()\n",
    "        reshape = tf.reshape(pooling2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, W_3) + b_3)\n",
    "        \n",
    "        return tf.matmul(hidden, W_4) + b_4\n",
    "    \n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, \n",
    "                                                                                                          logits=logits))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\\\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T08:03:22.503302Z",
     "start_time": "2019-09-28T08:03:22.499313Z"
    }
   },
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T08:14:36.849255Z",
     "start_time": "2019-09-28T08:03:33.839785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at epoch 1 and iter 2499: 0.307176\n",
      "Minibatch  train, validation: 87.5%, 85.8%\n",
      "Model saved\n",
      "Minibatch loss at epoch 1 and iter 4999: 0.226077\n",
      "Minibatch  train, validation: 93.8%, 87.4%\n",
      "Model saved\n",
      "Minibatch loss at epoch 1 and iter 7499: 0.314080\n",
      "Minibatch  train, validation: 87.5%, 87.8%\n",
      "Model saved\n",
      "Minibatch loss at epoch 1 and iter 9999: 0.732768\n",
      "Minibatch  train, validation: 87.5%, 88.9%\n",
      "Model saved\n",
      "Minibatch loss at epoch 1 and iter 12499: 0.634226\n",
      "Minibatch  train, validation: 87.5%, 89.3%\n",
      "Model saved\n",
      "Minibatch loss at epoch 2 and iter 14999: 0.274345\n",
      "Minibatch  train, validation: 87.5%, 89.0%\n",
      "Minibatch loss at epoch 2 and iter 17499: 0.089926\n",
      "Minibatch  train, validation: 93.8%, 89.5%\n",
      "Model saved\n",
      "Minibatch loss at epoch 2 and iter 19999: 0.305388\n",
      "Minibatch  train, validation: 87.5%, 89.7%\n",
      "Model saved\n",
      "Minibatch loss at epoch 2 and iter 22499: 0.569370\n",
      "Minibatch  train, validation: 87.5%, 89.9%\n",
      "Model saved\n",
      "Minibatch loss at epoch 2 and iter 24999: 0.568746\n",
      "Minibatch  train, validation: 81.2%, 90.1%\n",
      "Model saved\n",
      "Minibatch loss at epoch 3 and iter 27499: 0.276424\n",
      "Minibatch  train, validation: 87.5%, 89.6%\n",
      "Minibatch loss at epoch 3 and iter 29999: 0.051189\n",
      "Minibatch  train, validation: 100.0%, 90.3%\n",
      "Model saved\n",
      "Minibatch loss at epoch 3 and iter 32499: 0.399384\n",
      "Minibatch  train, validation: 87.5%, 90.4%\n",
      "Model saved\n",
      "Minibatch loss at epoch 3 and iter 34999: 0.503775\n",
      "Minibatch  train, validation: 87.5%, 90.3%\n",
      "Minibatch loss at epoch 3 and iter 37499: 0.497196\n",
      "Minibatch  train, validation: 81.2%, 90.5%\n",
      "Model saved\n",
      "Minibatch loss at epoch 4 and iter 39999: 0.273562\n",
      "Minibatch  train, validation: 87.5%, 89.8%\n",
      "Minibatch loss at epoch 4 and iter 42499: 0.047852\n",
      "Minibatch  train, validation: 100.0%, 90.1%\n",
      "Minibatch loss at epoch 4 and iter 44999: 0.312170\n",
      "Minibatch  train, validation: 87.5%, 90.6%\n",
      "Model saved\n",
      "Minibatch loss at epoch 4 and iter 47499: 0.484046\n",
      "Minibatch  train, validation: 87.5%, 90.5%\n",
      "Minibatch loss at epoch 4 and iter 49999: 0.455428\n",
      "Minibatch  train, validation: 81.2%, 90.5%\n",
      "Minibatch loss at epoch 5 and iter 52499: 0.261934\n",
      "Minibatch  train, validation: 87.5%, 90.0%\n",
      "Minibatch loss at epoch 5 and iter 54999: 0.018599\n",
      "Minibatch  train, validation: 100.0%, 90.5%\n",
      "Minibatch loss at epoch 5 and iter 57499: 0.247943\n",
      "Minibatch  train, validation: 87.5%, 90.6%\n",
      "Model saved\n",
      "Minibatch loss at epoch 5 and iter 59999: 0.493799\n",
      "Minibatch  train, validation: 87.5%, 90.8%\n",
      "Model saved\n",
      "Minibatch loss at epoch 5 and iter 62499: 0.428686\n",
      "Minibatch  train, validation: 87.5%, 90.6%\n",
      "Minibatch loss at epoch 6 and iter 64999: 0.274355\n",
      "Minibatch  train, validation: 87.5%, 90.3%\n",
      "Minibatch loss at epoch 6 and iter 67499: 0.032346\n",
      "Minibatch  train, validation: 100.0%, 90.7%\n",
      "Minibatch loss at epoch 6 and iter 69999: 0.206276\n",
      "Minibatch  train, validation: 93.8%, 90.5%\n",
      "Minibatch loss at epoch 6 and iter 72499: 0.484592\n",
      "Minibatch  train, validation: 87.5%, 90.9%\n",
      "Model saved\n",
      "Minibatch loss at epoch 6 and iter 74999: 0.448374\n",
      "Minibatch  train, validation: 87.5%, 90.8%\n",
      "Minibatch loss at epoch 7 and iter 77499: 0.296870\n",
      "Minibatch  train, validation: 87.5%, 90.1%\n",
      "Minibatch loss at epoch 7 and iter 79999: 0.013831\n",
      "Minibatch  train, validation: 100.0%, 90.9%\n",
      "Model saved\n",
      "Minibatch loss at epoch 7 and iter 82499: 0.212585\n",
      "Minibatch  train, validation: 87.5%, 90.5%\n",
      "Minibatch loss at epoch 7 and iter 84999: 0.498547\n",
      "Minibatch  train, validation: 87.5%, 90.7%\n",
      "Minibatch loss at epoch 7 and iter 87499: 0.405872\n",
      "Minibatch  train, validation: 93.8%, 90.6%\n",
      "Minibatch loss at epoch 8 and iter 89999: 0.327353\n",
      "Minibatch  train, validation: 87.5%, 90.3%\n",
      "Minibatch loss at epoch 8 and iter 92499: 0.005839\n",
      "Minibatch  train, validation: 100.0%, 90.9%\n",
      "Minibatch loss at epoch 8 and iter 94999: 0.190116\n",
      "Minibatch  train, validation: 93.8%, 90.6%\n",
      "Minibatch loss at epoch 8 and iter 97499: 0.495806\n",
      "Minibatch  train, validation: 87.5%, 90.8%\n",
      "Minibatch loss at epoch 8 and iter 99999: 0.361373\n",
      "Minibatch  train, validation: 93.8%, 90.7%\n",
      "Minibatch loss at epoch 9 and iter 102499: 0.318104\n",
      "Minibatch  train, validation: 87.5%, 90.5%\n",
      "Minibatch loss at epoch 9 and iter 104999: 0.006900\n",
      "Minibatch  train, validation: 100.0%, 91.0%\n",
      "Model saved\n",
      "Minibatch loss at epoch 9 and iter 107499: 0.208388\n",
      "Minibatch  train, validation: 93.8%, 90.8%\n",
      "Minibatch loss at epoch 9 and iter 109999: 0.452148\n",
      "Minibatch  train, validation: 87.5%, 90.6%\n",
      "Minibatch loss at epoch 9 and iter 112499: 0.293572\n",
      "Minibatch  train, validation: 93.8%, 90.7%\n",
      "Minibatch loss at epoch 10 and iter 114999: 0.337028\n",
      "Minibatch  train, validation: 87.5%, 90.5%\n",
      "Minibatch loss at epoch 10 and iter 117499: 0.001858\n",
      "Minibatch  train, validation: 100.0%, 90.8%\n",
      "Minibatch loss at epoch 10 and iter 119999: 0.158903\n",
      "Minibatch  train, validation: 93.8%, 90.5%\n",
      "Minibatch loss at epoch 10 and iter 122499: 0.414994\n",
      "Minibatch  train, validation: 87.5%, 90.5%\n",
      "Minibatch loss at epoch 10 and iter 124999: 0.309239\n",
      "Minibatch  train, validation: 93.8%, 90.5%\n",
      "Minibatch loss at epoch 11 and iter 127499: 0.317672\n",
      "Minibatch  train, validation: 87.5%, 90.5%\n",
      "Minibatch loss at epoch 11 and iter 129999: 0.002403\n",
      "Minibatch  train, validation: 100.0%, 91.0%\n",
      "Model saved\n",
      "Minibatch loss at epoch 11 and iter 132499: 0.151155\n",
      "Minibatch  train, validation: 93.8%, 90.6%\n",
      "Minibatch loss at epoch 11 and iter 134999: 0.482818\n",
      "Minibatch  train, validation: 87.5%, 90.2%\n",
      "Minibatch loss at epoch 11 and iter 137499: 0.244208\n",
      "Minibatch  train, validation: 93.8%, 90.3%\n",
      "Minibatch loss at epoch 12 and iter 139999: 0.315338\n",
      "Minibatch  train, validation: 87.5%, 90.3%\n",
      "Minibatch loss at epoch 12 and iter 142499: 0.000881\n",
      "Minibatch  train, validation: 100.0%, 90.5%\n",
      "Minibatch loss at epoch 12 and iter 144999: 0.167510\n",
      "Minibatch  train, validation: 93.8%, 90.5%\n",
      "Minibatch loss at epoch 12 and iter 147499: 0.357396\n",
      "Minibatch  train, validation: 87.5%, 90.3%\n",
      "Minibatch loss at epoch 12 and iter 149999: 0.192072\n",
      "Minibatch  train, validation: 87.5%, 90.5%\n",
      "Minibatch loss at epoch 13 and iter 152499: 0.314524\n",
      "Minibatch  train, validation: 87.5%, 90.6%\n",
      "Minibatch loss at epoch 13 and iter 154999: 0.002649\n",
      "Minibatch  train, validation: 100.0%, 90.6%\n",
      "Minibatch loss at epoch 13 and iter 157499: 0.187636\n",
      "Minibatch  train, validation: 93.8%, 90.7%\n",
      "Minibatch loss at epoch 13 and iter 159999: 0.428997\n",
      "Minibatch  train, validation: 87.5%, 90.5%\n",
      "Minibatch loss at epoch 13 and iter 162499: 0.074853\n",
      "Minibatch  train, validation: 100.0%, 90.1%\n",
      "Minibatch loss at epoch 14 and iter 164999: 0.353629\n",
      "Minibatch  train, validation: 87.5%, 90.3%\n",
      "Minibatch loss at epoch 14 and iter 167499: 0.000259\n",
      "Minibatch  train, validation: 100.0%, 90.6%\n",
      "Minibatch loss at epoch 14 and iter 169999: 0.191253\n",
      "Minibatch  train, validation: 93.8%, 90.5%\n",
      "Minibatch loss at epoch 14 and iter 172499: 0.419697\n",
      "Minibatch  train, validation: 87.5%, 90.3%\n",
      "Minibatch loss at epoch 14 and iter 174999: 0.131568\n",
      "Minibatch  train, validation: 93.8%, 90.1%\n",
      "Minibatch loss at epoch 15 and iter 177499: 0.318979\n",
      "Minibatch  train, validation: 87.5%, 90.3%\n",
      "Minibatch loss at epoch 15 and iter 179999: 0.000309\n",
      "Minibatch  train, validation: 100.0%, 90.5%\n",
      "Minibatch loss at epoch 15 and iter 182499: 0.134498\n",
      "Minibatch  train, validation: 93.8%, 90.8%\n",
      "Minibatch loss at epoch 15 and iter 184999: 0.426537\n",
      "Minibatch  train, validation: 87.5%, 90.2%\n",
      "Minibatch loss at epoch 15 and iter 187499: 0.216050\n",
      "Minibatch  train, validation: 93.8%, 90.0%\n",
      "Minibatch loss at epoch 16 and iter 189999: 0.338438\n",
      "Minibatch  train, validation: 87.5%, 90.2%\n",
      "Minibatch loss at epoch 16 and iter 192499: 0.000690\n",
      "Minibatch  train, validation: 100.0%, 90.2%\n",
      "Minibatch loss at epoch 16 and iter 194999: 0.195808\n",
      "Minibatch  train, validation: 93.8%, 90.5%\n",
      "Minibatch loss at epoch 16 and iter 197499: 0.394505\n",
      "Minibatch  train, validation: 87.5%, 90.1%\n",
      "Minibatch loss at epoch 16 and iter 199999: 0.149878\n",
      "Minibatch  train, validation: 93.8%, 90.2%\n",
      "Minibatch loss at epoch 17 and iter 202499: 0.354771\n",
      "Minibatch  train, validation: 87.5%, 90.3%\n",
      "Minibatch loss at epoch 17 and iter 204999: 0.003324\n",
      "Minibatch  train, validation: 100.0%, 90.0%\n",
      "Minibatch loss at epoch 17 and iter 207499: 0.155826\n",
      "Minibatch  train, validation: 93.8%, 90.3%\n",
      "Minibatch loss at epoch 17 and iter 209999: 0.471342\n",
      "Minibatch  train, validation: 87.5%, 90.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at epoch 17 and iter 212499: 0.177438\n",
      "Minibatch  train, validation: 93.8%, 89.8%\n",
      "Minibatch loss at epoch 18 and iter 214999: 0.302503\n",
      "Minibatch  train, validation: 87.5%, 90.4%\n",
      "Minibatch loss at epoch 18 and iter 217499: 0.003092\n",
      "Minibatch  train, validation: 100.0%, 90.2%\n",
      "Minibatch loss at epoch 18 and iter 219999: 0.097408\n",
      "Minibatch  train, validation: 100.0%, 90.5%\n",
      "Minibatch loss at epoch 18 and iter 222499: 0.493849\n",
      "Minibatch  train, validation: 87.5%, 90.2%\n",
      "Minibatch loss at epoch 18 and iter 224999: 0.100987\n",
      "Minibatch  train, validation: 100.0%, 89.9%\n",
      "Minibatch loss at epoch 19 and iter 227499: 0.294821\n",
      "Minibatch  train, validation: 87.5%, 89.9%\n",
      "Minibatch loss at epoch 19 and iter 229999: 0.005549\n",
      "Minibatch  train, validation: 100.0%, 90.1%\n",
      "Minibatch loss at epoch 19 and iter 232499: 0.125193\n",
      "Minibatch  train, validation: 93.8%, 90.4%\n",
      "Minibatch loss at epoch 19 and iter 234999: 0.434762\n",
      "Minibatch  train, validation: 87.5%, 90.1%\n",
      "Minibatch loss at epoch 19 and iter 237499: 0.172662\n",
      "Minibatch  train, validation: 93.8%, 89.7%\n",
      "Minibatch loss at epoch 20 and iter 239999: 0.319071\n",
      "Minibatch  train, validation: 87.5%, 90.1%\n",
      "Minibatch loss at epoch 20 and iter 242499: 0.001476\n",
      "Minibatch  train, validation: 100.0%, 90.0%\n",
      "Minibatch loss at epoch 20 and iter 244999: 0.159052\n",
      "Minibatch  train, validation: 87.5%, 90.4%\n",
      "Minibatch loss at epoch 20 and iter 247499: 0.441307\n",
      "Minibatch  train, validation: 87.5%, 90.0%\n",
      "Minibatch loss at epoch 20 and iter 249999: 0.156955\n",
      "Minibatch  train, validation: 93.8%, 89.7%\n",
      "Minibatch loss at epoch 21 and iter 252499: 0.310106\n",
      "Minibatch  train, validation: 87.5%, 90.0%\n",
      "Minibatch loss at epoch 21 and iter 254999: 0.000251\n",
      "Minibatch  train, validation: 100.0%, 90.1%\n",
      "Minibatch loss at epoch 21 and iter 257499: 0.189260\n",
      "Minibatch  train, validation: 93.8%, 90.1%\n",
      "Final Test accuracy: 95.6%\n",
      "Total run time 11.0496 minutes\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100 # number of epochs\n",
    "\n",
    "# early-stopping parameters\n",
    "patience = 5000 # look as this many examples regardless\n",
    "patience_increase = 2\n",
    "improvement_threshold = 0.995\n",
    "best_valid_loss = np.inf\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "n_train_batches = train_dataset.shape[0] // batch_size\n",
    "\n",
    "valid_freq = min(n_train_batches, patience // 2)\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "    \n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            batch_data =  train_dataset[minibatch_index * batch_size:(minibatch_index + 1) * batch_size, :]\n",
    "            batch_labels = train_labels[minibatch_index * batch_size:(minibatch_index + 1) * batch_size, :]\n",
    "            \n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = sess.run([optimizer, loss, train_prediction], \n",
    "                                                       feed_dict=feed_dict)\n",
    "            \n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index # cumulative iteration number\n",
    "            \n",
    "            if (iter + 1) % valid_freq == 0:\n",
    "                this_valid_loss = 1. - (accuracy(valid_prediction.eval(), valid_labels) / 100.)\n",
    "                \n",
    "                print(\"Minibatch loss at epoch %i and iter %i: %f\" % \n",
    "                      (epoch, iter, l))\n",
    "                print(\"Minibatch  train, validation: %.1f%%, %.1f%%\" %\n",
    "                    (accuracy(predictions, batch_labels), \n",
    "                    accuracy(valid_prediction.eval(), valid_labels)))\n",
    "                \n",
    "                if this_valid_loss < best_valid_loss:\n",
    "                    # 错误率在还在下降，将patience最大训练允许数增大\n",
    "                    if this_valid_loss < best_valid_loss * improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "                        \n",
    "                    best_valid_loss = this_valid_loss\n",
    "                    \n",
    "                    params = (sess.run(W_1), sess.run(b_1),\n",
    "                              sess.run(W_2), sess.run(b_2),\n",
    "                              sess.run(W_3), sess.run(b_3),\n",
    "                              sess.run(W_4), sess.run(b_4))\n",
    "                    \n",
    "                    # save the best model\n",
    "                    with open('best_model_params.pkl', 'wb') as f:\n",
    "                            pickle.dump(params, f)\n",
    "                    print('Model saved')\n",
    "        \n",
    "            if patience <= iter:\n",
    "                    done_looping = True\n",
    "                    break\n",
    "                    \n",
    "    print(\"Final Test accuracy: %.1f%%\" \n",
    "                  % accuracy(test_prediction.eval(), test_labels))\n",
    "    \n",
    "    end_time = timeit.default_timer()\n",
    "\n",
    "    print('Total run time %.4f minutes' % ((end_time - start_time) / 60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T08:16:30.654577Z",
     "start_time": "2019-09-28T08:16:27.609003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with the best model: 96.3%\n"
     ]
    }
   ],
   "source": [
    "# Load params:\n",
    "with tf.Session(graph=g) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    with open('best_model_params.pkl', 'rb') as f:\n",
    "        (W_1_best, b_1_best, W_2_best, b_2_best,\n",
    "         W_3_best, b_3_best, W_4_best, b_4_best) = pickle.load(f)\n",
    "\n",
    "        W_1_init, b_1_init = tf.assign(W_1, W_1_best), tf.assign(b_1, b_1_best)\n",
    "        W_2_init, b_2_init = tf.assign(W_2, W_2_best), tf.assign(b_2, b_2_best)\n",
    "        W_3_init, b_3_init = tf.assign(W_3, W_3_best), tf.assign(b_3, b_3_best)\n",
    "        W_4_init, b_4_init = tf.assign(W_4, W_4_best), tf.assign(b_4, b_4_best)\n",
    "\n",
    "        sess.run([W_1_init, b_1_init,\n",
    "                  W_2_init, b_2_init,\n",
    "                  W_3_init, b_3_init,\n",
    "                  W_4_init, b_4_init])\n",
    "\n",
    "        print(\"Test accuracy with the best model: %.1f%%\" \n",
    "                      % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet-5 model with dropouts and learning rate decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T08:34:36.646690Z",
     "start_time": "2019-09-28T08:34:35.873729Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "patch_size = 5\n",
    "\n",
    "g = tf.Graph()\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "\n",
    "with g.as_default():\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "      tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.1, global_step, 10000, 0.95, staircase=True)\n",
    "    \n",
    "    # Variables.\n",
    "    W_1 = tf.get_variable(\"W1\", shape=[patch_size, patch_size, num_channels, 6],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_1 = tf.Variable(tf.zeros([6]))\n",
    "    \n",
    "    W_2 = tf.get_variable(\"W2\", shape=[patch_size, patch_size, 6, 16],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_2 = tf.Variable(tf.zeros([16]))\n",
    "    \n",
    "    W_3 = tf.get_variable(\"W3\", shape=[5 * 5 * 16, 120],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_3 = tf.Variable(tf.zeros([120]))\n",
    "    \n",
    "    W_4 = tf.get_variable(\"W4\", shape=[120, 84],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_4 = tf.Variable(tf.zeros([84]))\n",
    "    \n",
    "    W_5 = tf.get_variable(\"W5\", shape=[84, num_labels],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_5 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Model.\n",
    "    def model(data, dropout=False):\n",
    "        conv = tf.nn.conv2d(data, W_1, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + b_1)\n",
    "        bn = tf.layers.batch_normalization(hidden)\n",
    "        pool = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        \n",
    "        conv = tf.nn.conv2d(pool, W_2, [1, 1, 1, 1], padding='VALID')\n",
    "        hidden = tf.nn.relu(conv + b_2)\n",
    "        bn = tf.layers.batch_normalization(hidden)\n",
    "        pool = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        \n",
    "        shape = pool.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, W_3) + b_3)\n",
    "        if dropout:\n",
    "            hidden = tf.nn.dropout(hidden, 0.5)\n",
    "\n",
    "        hidden = tf.nn.relu(tf.matmul(hidden, W_4) + b_4)\n",
    "        if dropout:\n",
    "            hidden = tf.nn.dropout(hidden, 0.5)\n",
    "        \n",
    "        return tf.matmul(hidden, W_5) + b_5\n",
    "    \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset, dropout=True)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "      \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, \n",
    "                                                                                                                      global_step=global_step)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T09:01:11.872864Z",
     "start_time": "2019-09-28T08:34:42.715689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at epoch 1 and iter 2499: 0.178449\n",
      "Minibatch  train, validation: 93.8%, 86.4%\n",
      "Model saved\n",
      "Minibatch loss at epoch 1 and iter 4999: 0.624195\n",
      "Minibatch  train, validation: 90.6%, 87.5%\n",
      "Model saved\n",
      "Minibatch loss at epoch 2 and iter 7499: 0.477033\n",
      "Minibatch  train, validation: 84.4%, 87.9%\n",
      "Model saved\n",
      "Minibatch loss at epoch 2 and iter 9999: 0.268128\n",
      "Minibatch  train, validation: 90.6%, 88.4%\n",
      "Model saved\n",
      "Minibatch loss at epoch 2 and iter 12499: 0.719805\n",
      "Minibatch  train, validation: 78.1%, 88.5%\n",
      "Model saved\n",
      "Minibatch loss at epoch 3 and iter 14999: 0.083738\n",
      "Minibatch  train, validation: 93.8%, 88.9%\n",
      "Model saved\n",
      "Minibatch loss at epoch 3 and iter 17499: 0.500426\n",
      "Minibatch  train, validation: 87.5%, 89.0%\n",
      "Model saved\n",
      "Minibatch loss at epoch 4 and iter 19999: 0.407011\n",
      "Minibatch  train, validation: 87.5%, 89.4%\n",
      "Model saved\n",
      "Minibatch loss at epoch 4 and iter 22499: 0.263930\n",
      "Minibatch  train, validation: 90.6%, 89.4%\n",
      "Model saved\n",
      "Minibatch loss at epoch 4 and iter 24999: 0.592775\n",
      "Minibatch  train, validation: 84.4%, 89.6%\n",
      "Model saved\n",
      "Minibatch loss at epoch 5 and iter 27499: 0.041062\n",
      "Minibatch  train, validation: 100.0%, 89.5%\n",
      "Minibatch loss at epoch 5 and iter 29999: 0.484224\n",
      "Minibatch  train, validation: 87.5%, 89.6%\n",
      "Minibatch loss at epoch 6 and iter 32499: 0.377761\n",
      "Minibatch  train, validation: 81.2%, 89.7%\n",
      "Model saved\n",
      "Minibatch loss at epoch 6 and iter 34999: 0.247072\n",
      "Minibatch  train, validation: 90.6%, 89.7%\n",
      "Minibatch loss at epoch 6 and iter 37499: 0.563442\n",
      "Minibatch  train, validation: 81.2%, 89.8%\n",
      "Model saved\n",
      "Minibatch loss at epoch 7 and iter 39999: 0.071272\n",
      "Minibatch  train, validation: 96.9%, 90.0%\n",
      "Model saved\n",
      "Minibatch loss at epoch 7 and iter 42499: 0.472145\n",
      "Minibatch  train, validation: 84.4%, 90.1%\n",
      "Model saved\n",
      "Minibatch loss at epoch 8 and iter 44999: 0.346642\n",
      "Minibatch  train, validation: 84.4%, 90.0%\n",
      "Minibatch loss at epoch 8 and iter 47499: 0.178344\n",
      "Minibatch  train, validation: 93.8%, 89.8%\n",
      "Minibatch loss at epoch 8 and iter 49999: 0.525772\n",
      "Minibatch  train, validation: 84.4%, 90.0%\n",
      "Minibatch loss at epoch 9 and iter 52499: 0.036729\n",
      "Minibatch  train, validation: 100.0%, 90.2%\n",
      "Model saved\n",
      "Minibatch loss at epoch 9 and iter 54999: 0.624400\n",
      "Minibatch  train, validation: 84.4%, 90.0%\n",
      "Minibatch loss at epoch 10 and iter 57499: 0.353418\n",
      "Minibatch  train, validation: 87.5%, 90.3%\n",
      "Model saved\n",
      "Minibatch loss at epoch 10 and iter 59999: 0.171146\n",
      "Minibatch  train, validation: 96.9%, 90.0%\n",
      "Minibatch loss at epoch 10 and iter 62499: 0.903532\n",
      "Minibatch  train, validation: 84.4%, 90.2%\n",
      "Minibatch loss at epoch 11 and iter 64999: 0.084321\n",
      "Minibatch  train, validation: 96.9%, 90.3%\n",
      "Model saved\n",
      "Minibatch loss at epoch 11 and iter 67499: 0.421349\n",
      "Minibatch  train, validation: 84.4%, 89.8%\n",
      "Minibatch loss at epoch 12 and iter 69999: 0.284101\n",
      "Minibatch  train, validation: 87.5%, 90.4%\n",
      "Model saved\n",
      "Minibatch loss at epoch 12 and iter 72499: 0.256001\n",
      "Minibatch  train, validation: 90.6%, 90.3%\n",
      "Minibatch loss at epoch 12 and iter 74999: 0.432744\n",
      "Minibatch  train, validation: 87.5%, 90.5%\n",
      "Model saved\n",
      "Minibatch loss at epoch 13 and iter 77499: 0.023203\n",
      "Minibatch  train, validation: 100.0%, 90.2%\n",
      "Minibatch loss at epoch 13 and iter 79999: 0.289619\n",
      "Minibatch  train, validation: 90.6%, 90.2%\n",
      "Minibatch loss at epoch 14 and iter 82499: 0.208848\n",
      "Minibatch  train, validation: 90.6%, 90.5%\n",
      "Model saved\n",
      "Minibatch loss at epoch 14 and iter 84999: 0.219390\n",
      "Minibatch  train, validation: 90.6%, 90.3%\n",
      "Minibatch loss at epoch 14 and iter 87499: 0.501536\n",
      "Minibatch  train, validation: 90.6%, 90.3%\n",
      "Minibatch loss at epoch 15 and iter 89999: 0.164939\n",
      "Minibatch  train, validation: 96.9%, 90.6%\n",
      "Model saved\n",
      "Minibatch loss at epoch 15 and iter 92499: 0.434065\n",
      "Minibatch  train, validation: 87.5%, 90.3%\n",
      "Minibatch loss at epoch 16 and iter 94999: 0.382529\n",
      "Minibatch  train, validation: 93.8%, 90.5%\n",
      "Minibatch loss at epoch 16 and iter 97499: 0.227845\n",
      "Minibatch  train, validation: 93.8%, 90.6%\n",
      "Model saved\n",
      "Minibatch loss at epoch 16 and iter 99999: 0.449012\n",
      "Minibatch  train, validation: 84.4%, 90.4%\n",
      "Minibatch loss at epoch 17 and iter 102499: 0.053147\n",
      "Minibatch  train, validation: 96.9%, 90.7%\n",
      "Model saved\n",
      "Minibatch loss at epoch 17 and iter 104999: 0.772768\n",
      "Minibatch  train, validation: 90.6%, 90.6%\n",
      "Minibatch loss at epoch 18 and iter 107499: 0.300410\n",
      "Minibatch  train, validation: 90.6%, 90.3%\n",
      "Minibatch loss at epoch 18 and iter 109999: 0.160596\n",
      "Minibatch  train, validation: 93.8%, 90.5%\n",
      "Minibatch loss at epoch 18 and iter 112499: 0.564696\n",
      "Minibatch  train, validation: 90.6%, 90.8%\n",
      "Model saved\n",
      "Minibatch loss at epoch 19 and iter 114999: 0.090877\n",
      "Minibatch  train, validation: 96.9%, 90.4%\n",
      "Minibatch loss at epoch 19 and iter 117499: 0.653827\n",
      "Minibatch  train, validation: 90.6%, 90.7%\n",
      "Minibatch loss at epoch 20 and iter 119999: 0.389484\n",
      "Minibatch  train, validation: 90.6%, 90.8%\n",
      "Minibatch loss at epoch 20 and iter 122499: 0.266843\n",
      "Minibatch  train, validation: 90.6%, 90.6%\n",
      "Minibatch loss at epoch 20 and iter 124999: 0.480954\n",
      "Minibatch  train, validation: 87.5%, 90.6%\n",
      "Minibatch loss at epoch 21 and iter 127499: 0.024079\n",
      "Minibatch  train, validation: 100.0%, 90.8%\n",
      "Minibatch loss at epoch 21 and iter 129999: 0.533651\n",
      "Minibatch  train, validation: 87.5%, 90.8%\n",
      "Minibatch loss at epoch 22 and iter 132499: 0.395355\n",
      "Minibatch  train, validation: 84.4%, 90.5%\n",
      "Minibatch loss at epoch 22 and iter 134999: 0.198463\n",
      "Minibatch  train, validation: 93.8%, 90.9%\n",
      "Model saved\n",
      "Minibatch loss at epoch 22 and iter 137499: 0.486722\n",
      "Minibatch  train, validation: 90.6%, 90.8%\n",
      "Minibatch loss at epoch 23 and iter 139999: 0.051599\n",
      "Minibatch  train, validation: 96.9%, 91.0%\n",
      "Model saved\n",
      "Minibatch loss at epoch 23 and iter 142499: 0.501428\n",
      "Minibatch  train, validation: 90.6%, 90.6%\n",
      "Minibatch loss at epoch 24 and iter 144999: 0.263816\n",
      "Minibatch  train, validation: 93.8%, 90.4%\n",
      "Minibatch loss at epoch 24 and iter 147499: 0.147328\n",
      "Minibatch  train, validation: 93.8%, 91.0%\n",
      "Minibatch loss at epoch 24 and iter 149999: 0.408518\n",
      "Minibatch  train, validation: 87.5%, 90.8%\n",
      "Minibatch loss at epoch 25 and iter 152499: 0.020141\n",
      "Minibatch  train, validation: 100.0%, 91.0%\n",
      "Model saved\n",
      "Minibatch loss at epoch 25 and iter 154999: 0.361218\n",
      "Minibatch  train, validation: 84.4%, 90.8%\n",
      "Minibatch loss at epoch 26 and iter 157499: 0.297565\n",
      "Minibatch  train, validation: 87.5%, 90.7%\n",
      "Minibatch loss at epoch 26 and iter 159999: 0.226353\n",
      "Minibatch  train, validation: 90.6%, 90.7%\n",
      "Minibatch loss at epoch 26 and iter 162499: 0.367178\n",
      "Minibatch  train, validation: 90.6%, 90.8%\n",
      "Minibatch loss at epoch 27 and iter 164999: 0.017078\n",
      "Minibatch  train, validation: 100.0%, 90.8%\n",
      "Minibatch loss at epoch 27 and iter 167499: 0.628756\n",
      "Minibatch  train, validation: 90.6%, 91.0%\n",
      "Minibatch loss at epoch 28 and iter 169999: 0.330665\n",
      "Minibatch  train, validation: 87.5%, 90.9%\n",
      "Minibatch loss at epoch 28 and iter 172499: 0.241013\n",
      "Minibatch  train, validation: 90.6%, 90.7%\n",
      "Minibatch loss at epoch 28 and iter 174999: 0.766831\n",
      "Minibatch  train, validation: 93.8%, 90.8%\n",
      "Minibatch loss at epoch 29 and iter 177499: 0.136162\n",
      "Minibatch  train, validation: 96.9%, 90.9%\n",
      "Minibatch loss at epoch 29 and iter 179999: 0.379186\n",
      "Minibatch  train, validation: 93.8%, 90.9%\n",
      "Minibatch loss at epoch 30 and iter 182499: 0.266270\n",
      "Minibatch  train, validation: 90.6%, 90.9%\n",
      "Minibatch loss at epoch 30 and iter 184999: 0.154875\n",
      "Minibatch  train, validation: 93.8%, 90.8%\n",
      "Minibatch loss at epoch 30 and iter 187499: 0.505529\n",
      "Minibatch  train, validation: 90.6%, 90.8%\n",
      "Minibatch loss at epoch 31 and iter 189999: 0.111645\n",
      "Minibatch  train, validation: 96.9%, 91.1%\n",
      "Model saved\n",
      "Minibatch loss at epoch 31 and iter 192499: 0.400879\n",
      "Minibatch  train, validation: 87.5%, 91.0%\n",
      "Minibatch loss at epoch 32 and iter 194999: 0.208079\n",
      "Minibatch  train, validation: 93.8%, 90.9%\n",
      "Minibatch loss at epoch 32 and iter 197499: 0.174402\n",
      "Minibatch  train, validation: 93.8%, 90.6%\n",
      "Minibatch loss at epoch 32 and iter 199999: 0.335015\n",
      "Minibatch  train, validation: 87.5%, 91.0%\n",
      "Minibatch loss at epoch 33 and iter 202499: 0.137331\n",
      "Minibatch  train, validation: 96.9%, 91.0%\n",
      "Minibatch loss at epoch 33 and iter 204999: 0.592189\n",
      "Minibatch  train, validation: 93.8%, 90.8%\n",
      "Minibatch loss at epoch 34 and iter 207499: 0.255159\n",
      "Minibatch  train, validation: 90.6%, 91.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at epoch 34 and iter 209999: 0.200664\n",
      "Minibatch  train, validation: 90.6%, 90.9%\n",
      "Minibatch loss at epoch 34 and iter 212499: 0.466235\n",
      "Minibatch  train, validation: 87.5%, 91.0%\n",
      "Minibatch loss at epoch 35 and iter 214999: 0.013146\n",
      "Minibatch  train, validation: 100.0%, 91.0%\n",
      "Minibatch loss at epoch 35 and iter 217499: 0.455872\n",
      "Minibatch  train, validation: 93.8%, 91.0%\n",
      "Minibatch loss at epoch 36 and iter 219999: 0.271916\n",
      "Minibatch  train, validation: 93.8%, 90.8%\n",
      "Minibatch loss at epoch 36 and iter 222499: 0.119920\n",
      "Minibatch  train, validation: 96.9%, 90.9%\n",
      "Minibatch loss at epoch 36 and iter 224999: 0.347057\n",
      "Minibatch  train, validation: 93.8%, 91.0%\n",
      "Minibatch loss at epoch 37 and iter 227499: 0.002287\n",
      "Minibatch  train, validation: 100.0%, 90.8%\n",
      "Minibatch loss at epoch 37 and iter 229999: 0.577586\n",
      "Minibatch  train, validation: 87.5%, 91.0%\n",
      "Minibatch loss at epoch 38 and iter 232499: 0.203466\n",
      "Minibatch  train, validation: 90.6%, 90.8%\n",
      "Minibatch loss at epoch 38 and iter 234999: 0.181318\n",
      "Minibatch  train, validation: 93.8%, 90.9%\n",
      "Minibatch loss at epoch 38 and iter 237499: 0.299114\n",
      "Minibatch  train, validation: 87.5%, 91.2%\n",
      "Model saved\n",
      "Minibatch loss at epoch 39 and iter 239999: 0.119280\n",
      "Minibatch  train, validation: 96.9%, 90.8%\n",
      "Minibatch loss at epoch 39 and iter 242499: 0.386825\n",
      "Minibatch  train, validation: 90.6%, 91.0%\n",
      "Minibatch loss at epoch 40 and iter 244999: 0.229234\n",
      "Minibatch  train, validation: 93.8%, 90.8%\n",
      "Minibatch loss at epoch 40 and iter 247499: 0.171124\n",
      "Minibatch  train, validation: 93.8%, 90.9%\n",
      "Minibatch loss at epoch 40 and iter 249999: 0.670089\n",
      "Minibatch  train, validation: 90.6%, 90.8%\n",
      "Minibatch loss at epoch 41 and iter 252499: 0.026757\n",
      "Minibatch  train, validation: 100.0%, 91.1%\n",
      "Minibatch loss at epoch 41 and iter 254999: 0.336871\n",
      "Minibatch  train, validation: 96.9%, 91.1%\n",
      "Minibatch loss at epoch 42 and iter 257499: 0.250329\n",
      "Minibatch  train, validation: 90.6%, 91.0%\n",
      "Minibatch loss at epoch 42 and iter 259999: 0.165418\n",
      "Minibatch  train, validation: 93.8%, 90.8%\n",
      "Minibatch loss at epoch 42 and iter 262499: 0.497744\n",
      "Minibatch  train, validation: 93.8%, 91.0%\n",
      "Minibatch loss at epoch 43 and iter 264999: 0.092568\n",
      "Minibatch  train, validation: 96.9%, 91.1%\n",
      "Minibatch loss at epoch 43 and iter 267499: 0.500248\n",
      "Minibatch  train, validation: 90.6%, 91.0%\n",
      "Minibatch loss at epoch 44 and iter 269999: 0.358240\n",
      "Minibatch  train, validation: 84.4%, 90.7%\n",
      "Minibatch loss at epoch 44 and iter 272499: 0.198475\n",
      "Minibatch  train, validation: 93.8%, 91.0%\n",
      "Minibatch loss at epoch 44 and iter 274999: 0.652523\n",
      "Minibatch  train, validation: 90.6%, 91.1%\n",
      "Minibatch loss at epoch 45 and iter 277499: 0.015137\n",
      "Minibatch  train, validation: 100.0%, 91.0%\n",
      "Minibatch loss at epoch 45 and iter 279999: 0.354962\n",
      "Minibatch  train, validation: 90.6%, 91.1%\n",
      "Minibatch loss at epoch 46 and iter 282499: 0.186770\n",
      "Minibatch  train, validation: 90.6%, 90.9%\n",
      "Minibatch loss at epoch 46 and iter 284999: 0.205344\n",
      "Minibatch  train, validation: 93.8%, 91.0%\n",
      "Minibatch loss at epoch 46 and iter 287499: 0.608519\n",
      "Minibatch  train, validation: 90.6%, 91.0%\n",
      "Minibatch loss at epoch 47 and iter 289999: 0.150634\n",
      "Minibatch  train, validation: 93.8%, 91.2%\n",
      "Model saved\n",
      "Minibatch loss at epoch 47 and iter 292499: 0.354178\n",
      "Minibatch  train, validation: 90.6%, 91.2%\n",
      "Model saved\n",
      "Minibatch loss at epoch 48 and iter 294999: 0.232211\n",
      "Minibatch  train, validation: 90.6%, 91.2%\n",
      "Minibatch loss at epoch 48 and iter 297499: 0.145456\n",
      "Minibatch  train, validation: 93.8%, 91.2%\n",
      "Minibatch loss at epoch 48 and iter 299999: 0.353688\n",
      "Minibatch  train, validation: 90.6%, 91.1%\n",
      "Minibatch loss at epoch 49 and iter 302499: 0.040099\n",
      "Minibatch  train, validation: 96.9%, 91.0%\n",
      "Minibatch loss at epoch 49 and iter 304999: 0.579338\n",
      "Minibatch  train, validation: 87.5%, 91.2%\n",
      "Model saved\n",
      "Minibatch loss at epoch 50 and iter 307499: 0.233063\n",
      "Minibatch  train, validation: 90.6%, 90.9%\n",
      "Minibatch loss at epoch 50 and iter 309999: 0.148231\n",
      "Minibatch  train, validation: 93.8%, 91.1%\n",
      "Minibatch loss at epoch 50 and iter 312499: 0.573302\n",
      "Minibatch  train, validation: 90.6%, 91.1%\n",
      "Minibatch loss at epoch 51 and iter 314999: 0.019682\n",
      "Minibatch  train, validation: 100.0%, 91.1%\n",
      "Minibatch loss at epoch 51 and iter 317499: 0.319594\n",
      "Minibatch  train, validation: 90.6%, 91.1%\n",
      "Minibatch loss at epoch 52 and iter 319999: 0.200365\n",
      "Minibatch  train, validation: 90.6%, 91.0%\n",
      "Minibatch loss at epoch 52 and iter 322499: 0.231373\n",
      "Minibatch  train, validation: 90.6%, 91.2%\n",
      "Minibatch loss at epoch 52 and iter 324999: 0.738260\n",
      "Minibatch  train, validation: 90.6%, 91.0%\n",
      "Minibatch loss at epoch 53 and iter 327499: 0.052296\n",
      "Minibatch  train, validation: 96.9%, 91.3%\n",
      "Model saved\n",
      "Minibatch loss at epoch 53 and iter 329999: 0.341604\n",
      "Minibatch  train, validation: 93.8%, 91.0%\n",
      "Minibatch loss at epoch 54 and iter 332499: 0.166107\n",
      "Minibatch  train, validation: 96.9%, 91.1%\n",
      "Minibatch loss at epoch 54 and iter 334999: 0.173274\n",
      "Minibatch  train, validation: 96.9%, 91.0%\n",
      "Minibatch loss at epoch 54 and iter 337499: 0.490222\n",
      "Minibatch  train, validation: 93.8%, 91.2%\n",
      "Minibatch loss at epoch 55 and iter 339999: 0.080838\n",
      "Minibatch  train, validation: 96.9%, 91.2%\n",
      "Minibatch loss at epoch 55 and iter 342499: 0.309673\n",
      "Minibatch  train, validation: 90.6%, 91.0%\n",
      "Minibatch loss at epoch 56 and iter 344999: 0.170382\n",
      "Minibatch  train, validation: 93.8%, 91.0%\n",
      "Minibatch loss at epoch 56 and iter 347499: 0.151400\n",
      "Minibatch  train, validation: 93.8%, 91.0%\n",
      "Minibatch loss at epoch 56 and iter 349999: 0.311802\n",
      "Minibatch  train, validation: 90.6%, 91.0%\n",
      "Minibatch loss at epoch 57 and iter 352499: 0.105229\n",
      "Minibatch  train, validation: 93.8%, 91.0%\n",
      "Minibatch loss at epoch 57 and iter 354999: 0.310475\n",
      "Minibatch  train, validation: 90.6%, 91.0%\n",
      "Minibatch loss at epoch 58 and iter 357499: 0.275078\n",
      "Minibatch  train, validation: 87.5%, 91.1%\n",
      "Minibatch loss at epoch 58 and iter 359999: 0.153254\n",
      "Minibatch  train, validation: 93.8%, 91.0%\n",
      "Minibatch loss at epoch 58 and iter 362499: 0.370718\n",
      "Minibatch  train, validation: 90.6%, 91.1%\n",
      "Minibatch loss at epoch 59 and iter 364999: 0.054636\n",
      "Minibatch  train, validation: 96.9%, 91.1%\n",
      "Minibatch loss at epoch 59 and iter 367499: 0.333263\n",
      "Minibatch  train, validation: 93.8%, 91.2%\n",
      "Minibatch loss at epoch 60 and iter 369999: 0.219857\n",
      "Minibatch  train, validation: 90.6%, 91.0%\n",
      "Minibatch loss at epoch 60 and iter 372499: 0.166785\n",
      "Minibatch  train, validation: 96.9%, 91.1%\n",
      "Minibatch loss at epoch 60 and iter 374999: 0.404883\n",
      "Minibatch  train, validation: 81.2%, 91.1%\n",
      "Minibatch loss at epoch 61 and iter 377499: 0.033060\n",
      "Minibatch  train, validation: 100.0%, 91.1%\n",
      "Minibatch loss at epoch 61 and iter 379999: 0.194172\n",
      "Minibatch  train, validation: 93.8%, 91.1%\n",
      "Minibatch loss at epoch 62 and iter 382499: 0.223492\n",
      "Minibatch  train, validation: 90.6%, 91.1%\n",
      "Minibatch loss at epoch 62 and iter 384999: 0.275965\n",
      "Minibatch  train, validation: 90.6%, 91.0%\n",
      "Minibatch loss at epoch 62 and iter 387499: 0.500664\n",
      "Minibatch  train, validation: 90.6%, 91.1%\n",
      "Minibatch loss at epoch 63 and iter 389999: 0.035521\n",
      "Minibatch  train, validation: 100.0%, 91.2%\n",
      "Minibatch loss at epoch 63 and iter 392499: 0.265443\n",
      "Minibatch  train, validation: 93.8%, 91.0%\n",
      "Minibatch loss at epoch 64 and iter 394999: 0.309459\n",
      "Minibatch  train, validation: 90.6%, 91.1%\n",
      "Minibatch loss at epoch 64 and iter 397499: 0.175038\n",
      "Minibatch  train, validation: 90.6%, 91.0%\n",
      "Minibatch loss at epoch 64 and iter 399999: 0.844306\n",
      "Minibatch  train, validation: 90.6%, 91.1%\n",
      "Minibatch loss at epoch 65 and iter 402499: 0.167031\n",
      "Minibatch  train, validation: 96.9%, 91.2%\n",
      "Minibatch loss at epoch 65 and iter 404999: 0.291346\n",
      "Minibatch  train, validation: 90.6%, 91.1%\n",
      "Minibatch loss at epoch 66 and iter 407499: 0.303137\n",
      "Minibatch  train, validation: 90.6%, 91.1%\n",
      "Minibatch loss at epoch 66 and iter 409999: 0.228883\n",
      "Minibatch  train, validation: 90.6%, 90.9%\n",
      "Minibatch loss at epoch 66 and iter 412499: 0.247194\n",
      "Minibatch  train, validation: 93.8%, 91.1%\n",
      "Minibatch loss at epoch 67 and iter 414999: 0.016353\n",
      "Minibatch  train, validation: 100.0%, 91.2%\n",
      "Minibatch loss at epoch 67 and iter 417499: 0.283488\n",
      "Minibatch  train, validation: 90.6%, 91.0%\n",
      "Minibatch loss at epoch 68 and iter 419999: 0.220756\n",
      "Minibatch  train, validation: 90.6%, 91.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at epoch 68 and iter 422499: 0.162259\n",
      "Minibatch  train, validation: 96.9%, 91.1%\n",
      "Minibatch loss at epoch 68 and iter 424999: 0.449821\n",
      "Minibatch  train, validation: 96.9%, 91.1%\n",
      "Minibatch loss at epoch 69 and iter 427499: 0.055648\n",
      "Minibatch  train, validation: 96.9%, 91.1%\n",
      "Minibatch loss at epoch 69 and iter 429999: 0.282444\n",
      "Minibatch  train, validation: 93.8%, 91.2%\n",
      "Minibatch loss at epoch 70 and iter 432499: 0.210577\n",
      "Minibatch  train, validation: 93.8%, 91.1%\n",
      "Minibatch loss at epoch 70 and iter 434999: 0.198042\n",
      "Minibatch  train, validation: 93.8%, 91.0%\n",
      "Minibatch loss at epoch 70 and iter 437499: 0.761461\n",
      "Minibatch  train, validation: 93.8%, 91.0%\n",
      "Minibatch loss at epoch 71 and iter 439999: 0.025828\n",
      "Minibatch  train, validation: 100.0%, 91.1%\n",
      "Minibatch loss at epoch 71 and iter 442499: 0.327994\n",
      "Minibatch  train, validation: 90.6%, 91.1%\n",
      "Minibatch loss at epoch 72 and iter 444999: 0.215878\n",
      "Minibatch  train, validation: 93.8%, 91.0%\n",
      "Minibatch loss at epoch 72 and iter 447499: 0.194062\n",
      "Minibatch  train, validation: 100.0%, 91.1%\n",
      "Minibatch loss at epoch 72 and iter 449999: 0.245606\n",
      "Minibatch  train, validation: 90.6%, 91.2%\n",
      "Minibatch loss at epoch 73 and iter 452499: 0.102245\n",
      "Minibatch  train, validation: 96.9%, 91.3%\n",
      "Minibatch loss at epoch 73 and iter 454999: 0.312721\n",
      "Minibatch  train, validation: 87.5%, 91.1%\n",
      "Minibatch loss at epoch 74 and iter 457499: 0.191940\n",
      "Minibatch  train, validation: 96.9%, 91.2%\n",
      "Minibatch loss at epoch 74 and iter 459999: 0.169527\n",
      "Minibatch  train, validation: 93.8%, 91.1%\n",
      "Minibatch loss at epoch 74 and iter 462499: 0.426546\n",
      "Minibatch  train, validation: 90.6%, 91.1%\n",
      "Minibatch loss at epoch 75 and iter 464999: 0.031352\n",
      "Minibatch  train, validation: 100.0%, 91.2%\n",
      "Minibatch loss at epoch 75 and iter 467499: 0.291897\n",
      "Minibatch  train, validation: 90.6%, 91.2%\n",
      "Minibatch loss at epoch 76 and iter 469999: 0.280115\n",
      "Minibatch  train, validation: 90.6%, 91.0%\n",
      "Minibatch loss at epoch 76 and iter 472499: 0.140519\n",
      "Minibatch  train, validation: 96.9%, 91.2%\n",
      "Minibatch loss at epoch 76 and iter 474999: 0.422941\n",
      "Minibatch  train, validation: 84.4%, 91.1%\n",
      "Minibatch loss at epoch 77 and iter 477499: 0.011094\n",
      "Minibatch  train, validation: 100.0%, 91.2%\n",
      "Minibatch loss at epoch 77 and iter 479999: 0.215051\n",
      "Minibatch  train, validation: 90.6%, 91.0%\n",
      "Minibatch loss at epoch 78 and iter 482499: 0.131607\n",
      "Minibatch  train, validation: 93.8%, 91.1%\n",
      "Minibatch loss at epoch 78 and iter 484999: 0.176943\n",
      "Minibatch  train, validation: 93.8%, 91.0%\n",
      "Minibatch loss at epoch 78 and iter 487499: 0.408730\n",
      "Minibatch  train, validation: 84.4%, 91.0%\n",
      "Minibatch loss at epoch 79 and iter 489999: 0.033372\n",
      "Minibatch  train, validation: 96.9%, 91.1%\n",
      "Minibatch loss at epoch 79 and iter 492499: 0.423606\n",
      "Minibatch  train, validation: 87.5%, 91.1%\n",
      "Minibatch loss at epoch 80 and iter 494999: 0.264175\n",
      "Minibatch  train, validation: 84.4%, 91.1%\n",
      "Minibatch loss at epoch 80 and iter 497499: 0.119576\n",
      "Minibatch  train, validation: 96.9%, 91.1%\n",
      "Minibatch loss at epoch 80 and iter 499999: 0.251987\n",
      "Minibatch  train, validation: 96.9%, 91.1%\n",
      "Minibatch loss at epoch 81 and iter 502499: 0.028024\n",
      "Minibatch  train, validation: 100.0%, 91.1%\n",
      "Minibatch loss at epoch 81 and iter 504999: 0.182568\n",
      "Minibatch  train, validation: 93.8%, 91.0%\n",
      "Minibatch loss at epoch 82 and iter 507499: 0.216004\n",
      "Minibatch  train, validation: 93.8%, 91.0%\n",
      "Minibatch loss at epoch 82 and iter 509999: 0.236798\n",
      "Minibatch  train, validation: 87.5%, 91.0%\n",
      "Minibatch loss at epoch 82 and iter 512499: 0.650351\n",
      "Minibatch  train, validation: 90.6%, 91.2%\n",
      "Minibatch loss at epoch 83 and iter 514999: 0.032129\n",
      "Minibatch  train, validation: 100.0%, 91.1%\n",
      "Minibatch loss at epoch 83 and iter 517499: 0.291264\n",
      "Minibatch  train, validation: 87.5%, 91.1%\n",
      "Minibatch loss at epoch 84 and iter 519999: 0.197025\n",
      "Minibatch  train, validation: 93.8%, 91.0%\n",
      "Minibatch loss at epoch 84 and iter 522499: 0.175186\n",
      "Minibatch  train, validation: 96.9%, 91.1%\n",
      "Minibatch loss at epoch 84 and iter 524999: 0.437139\n",
      "Minibatch  train, validation: 87.5%, 91.2%\n",
      "Minibatch loss at epoch 85 and iter 527499: 0.019279\n",
      "Minibatch  train, validation: 100.0%, 91.1%\n",
      "Minibatch loss at epoch 85 and iter 529999: 0.385311\n",
      "Minibatch  train, validation: 87.5%, 91.0%\n",
      "Minibatch loss at epoch 86 and iter 532499: 0.191743\n",
      "Minibatch  train, validation: 93.8%, 91.0%\n",
      "Minibatch loss at epoch 86 and iter 534999: 0.157322\n",
      "Minibatch  train, validation: 93.8%, 91.0%\n",
      "Minibatch loss at epoch 86 and iter 537499: 0.593062\n",
      "Minibatch  train, validation: 87.5%, 91.1%\n",
      "Minibatch loss at epoch 87 and iter 539999: 0.007981\n",
      "Minibatch  train, validation: 100.0%, 91.1%\n",
      "Minibatch loss at epoch 87 and iter 542499: 0.303387\n",
      "Minibatch  train, validation: 87.5%, 91.1%\n",
      "Minibatch loss at epoch 88 and iter 544999: 0.214786\n",
      "Minibatch  train, validation: 87.5%, 91.1%\n",
      "Minibatch loss at epoch 88 and iter 547499: 0.168037\n",
      "Minibatch  train, validation: 93.8%, 91.0%\n",
      "Minibatch loss at epoch 88 and iter 549999: 0.282948\n",
      "Minibatch  train, validation: 90.6%, 91.1%\n",
      "Minibatch loss at epoch 89 and iter 552499: 0.019541\n",
      "Minibatch  train, validation: 100.0%, 91.1%\n",
      "Minibatch loss at epoch 89 and iter 554999: 0.299656\n",
      "Minibatch  train, validation: 87.5%, 91.1%\n",
      "Minibatch loss at epoch 90 and iter 557499: 0.221785\n",
      "Minibatch  train, validation: 87.5%, 91.1%\n",
      "Minibatch loss at epoch 90 and iter 559999: 0.129590\n",
      "Minibatch  train, validation: 93.8%, 91.0%\n",
      "Minibatch loss at epoch 90 and iter 562499: 0.221558\n",
      "Minibatch  train, validation: 90.6%, 91.1%\n",
      "Minibatch loss at epoch 91 and iter 564999: 0.004146\n",
      "Minibatch  train, validation: 100.0%, 91.0%\n",
      "Minibatch loss at epoch 91 and iter 567499: 0.281409\n",
      "Minibatch  train, validation: 90.6%, 91.0%\n",
      "Minibatch loss at epoch 92 and iter 569999: 0.141804\n",
      "Minibatch  train, validation: 93.8%, 91.0%\n",
      "Minibatch loss at epoch 92 and iter 572499: 0.134029\n",
      "Minibatch  train, validation: 93.8%, 91.0%\n",
      "Minibatch loss at epoch 92 and iter 574999: 0.371172\n",
      "Minibatch  train, validation: 93.8%, 91.1%\n",
      "Minibatch loss at epoch 93 and iter 577499: 0.025510\n",
      "Minibatch  train, validation: 100.0%, 91.1%\n",
      "Minibatch loss at epoch 93 and iter 579999: 0.341133\n",
      "Minibatch  train, validation: 90.6%, 91.1%\n",
      "Minibatch loss at epoch 94 and iter 582499: 0.277957\n",
      "Minibatch  train, validation: 90.6%, 91.0%\n",
      "Final Test accuracy: 96.6%\n",
      "Total run time 26.4853 minutes\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100 # number of epochs\n",
    "\n",
    "# early-stopping parameters\n",
    "patience = 5000 # look as this many examples regardless\n",
    "patience_increase = 2\n",
    "improvement_threshold = 0.995\n",
    "best_valid_loss = np.inf\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "n_train_batches = train_dataset.shape[0] // batch_size\n",
    "\n",
    "valid_freq = min(n_train_batches, patience // 2)\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "    \n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            batch_data =  train_dataset[minibatch_index * batch_size:(minibatch_index + 1) * batch_size, :]\n",
    "            batch_labels = train_labels[minibatch_index * batch_size:(minibatch_index + 1) * batch_size, :]\n",
    "            \n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = sess.run([optimizer, loss, train_prediction], \n",
    "                                                       feed_dict=feed_dict)\n",
    "            \n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index # cumulative iteration number\n",
    "            \n",
    "            if (iter + 1) % valid_freq == 0:\n",
    "                this_valid_loss = 1. - (accuracy(valid_prediction.eval(), valid_labels) / 100.)\n",
    "                \n",
    "                print(\"Minibatch loss at epoch %i and iter %i: %f\" % \n",
    "                      (epoch, iter, l))\n",
    "                print(\"Minibatch  train, validation: %.1f%%, %.1f%%\" %\n",
    "                    (accuracy(predictions, batch_labels), \n",
    "                    accuracy(valid_prediction.eval(), valid_labels)))\n",
    "                \n",
    "                if this_valid_loss < best_valid_loss:\n",
    "                    # 错误率在还在下降，将patience最大训练允许数增大\n",
    "                    if this_valid_loss < best_valid_loss * improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "                        \n",
    "                    best_valid_loss = this_valid_loss\n",
    "                    \n",
    "                    params = (sess.run(W_1), sess.run(b_1),\n",
    "                              sess.run(W_2), sess.run(b_2),\n",
    "                              sess.run(W_3), sess.run(b_3),\n",
    "                              sess.run(W_4), sess.run(b_4),\n",
    "                              sess.run(W_5), sess.run(b_5),)\n",
    "                    \n",
    "                    # save the best model\n",
    "                    with open('best_model_params.pkl', 'wb') as f:\n",
    "                            pickle.dump(params, f)\n",
    "                    print('Model saved')\n",
    "        \n",
    "            if patience <= iter:\n",
    "                    done_looping = True\n",
    "                    break\n",
    "                    \n",
    "    print(\"Final Test accuracy: %.1f%%\" \n",
    "                  % accuracy(test_prediction.eval(), test_labels))\n",
    "    \n",
    "    end_time = timeit.default_timer()\n",
    "\n",
    "    print('Total run time %.4f minutes' % ((end_time - start_time) / 60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T09:03:51.225308Z",
     "start_time": "2019-09-28T09:03:50.615807Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load params:\n",
    "with tf.Session(graph=g) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    with open('best_model_params.pkl', 'rb') as f:\n",
    "        (W_1_best, b_1_best, W_2_best, b_2_best,\n",
    "         W_3_best, b_3_best, W_4_best, b_4_best,\n",
    "         W_5_best, b_5_best) = pickle.load(f)\n",
    "\n",
    "        W_1_init, b_1_init = tf.assign(W_1, W_1_best), tf.assign(b_1, b_1_best)\n",
    "        W_2_init, b_2_init = tf.assign(W_2, W_2_best), tf.assign(b_2, b_2_best)\n",
    "        W_3_init, b_3_init = tf.assign(W_3, W_3_best), tf.assign(b_3, b_3_best)\n",
    "        W_4_init, b_4_init = tf.assign(W_4, W_4_best), tf.assign(b_4, b_4_best)\n",
    "        W_5_init, b_5_init = tf.assign(W_5, W_5_best), tf.assign(b_5, b_5_best)\n",
    "\n",
    "        sess.run([W_1_init, b_1_init,\n",
    "                  W_2_init, b_2_init,\n",
    "                  W_3_init, b_3_init,\n",
    "                  W_4_init, b_4_init,\n",
    "                  W_5_init, b_5_init])\n",
    "\n",
    "        print(\"Test accuracy with the best model: %.1f%%\" \n",
    "                      % accuracy(test_prediction.eval(), test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-kernel",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
